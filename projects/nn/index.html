<!DOCTYPE html>

<html lang="en">
<head>
    <meta charset="utf-8"/>
    <title>Data Migration Case Study | Prathmesh Honrao</title>
    <meta content="width=device-width, initial-scale=1" name="viewport"/>
    <link href="../../style.css" rel="stylesheet"/>
    <link rel="icon" type="image/png" href="../../assets/favicon.png">
    <style>
        body {
            font: 16px/1.6 -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial;
            color: #111;
            background-color: #fff;
            margin: 0;
            padding: 20px;
            line-height: 1.6;
        }
        .container {
          max-width: 900px;
          margin: auto;
        }
        h1, h2, h3 {
          color: #1a1a1a;
        }
        hr {
          border: none;
          border-top: 1px solid #ccc;
          margin: 40px 0;
        }
        table {
          width: 100%;
          border-collapse: collapse;
          margin-top: 20px;
          font-size: 15px;
        }
        th, td {
          border: 1px solid #ddd;
          padding: 12px;
          text-align: left;
        }
        th {
          background-color: #f8f8f8;
        }
        img {
          max-width: 100%;
          margin: 20px 0;
          border-radius: 8px;
        }
        .tech-section {
            margin: 30px 0;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 8px;
            border-left: 4px solid #0066cc;
        }
        
        .tech-section h3 {
            color: #2c3e50;
            margin-top: 0;
        }
        
        .tech-section pre {
            background: #fff;
            padding: 15px;
            border-radius: 4px;
            overflow-x: auto;
            border: 1px solid #e9ecef;
        }
        
        .tech-section code {
            font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
            font-size: 14px;
        }

        .details {
            margin-top: 15px;
        }

        .details ul {
            margin: 10px 0;
        }

        .details ul li {
            margin: 5px 0;
        }
      </style>
</head>
<body>
<div class="container">
<a href="../../index.html">‚Üê Back to Home</a>
<h1>Data Migration for a Large Insurance Company in the Netherlands</h1>
<p><strong>Duration:</strong> Multi-phase rollout over 2 years</p>
<p><strong>Role:</strong> Lead ‚Äì Data Migration Workstream</p>
<p><strong>Technologies:</strong> COBOL, DB2, IMS, VSAM, Oracle, MS SQL Server, Java, Bash, Python, FTP, SQL*Loader</p>
<hr/>
<h2>Overview</h2>
<p>
      As part of a large-scale mainframe modernization initiative, I led the end-to-end data migration workstream 
      for a major insurance provider in the Netherlands. This transformation involved the migration of legacy 
      systems‚ÄîCOBOL, DB2, IMS, and VSAM‚Äîto modern x86 infrastructure using Java, Oracle, and MS SQL Server.
    </p>
<p>
      My work encompassed everything from architecture and planning to automation, testing, and go-live execution 
      across 170 applications and three LPARs‚Äîtransferring over 2500 GB of production data while ensuring accuracy,
      integrity, and audit traceability.
    </p>
<img alt="Data Migration Pipeline Diagram" src="nn_data_pipeline.png"/>
<img alt="Validation Model Diagram" src="nn_validation_model.png"/>
<hr/>
<h2>My Responsibilities</h2>
<ul>
<li>Architected the entire data migration pipeline from mainframe to Oracle/MS SQL</li>
<li>Developed Python and Bash scripts for DB2 to Oracle DDL conversion</li>
<li>Automated DB2 data unload using VSAM Utility and configured high-performance unload jobs</li>
<li>Built repeatable pipelines for data movement via FTP and NFS-mounted migration servers</li>
<li>Designed a 3-layer data validation strategy: row count, SHA-256 hash matching, and deep diff</li>
<li>Handled special data types: LOBs, CLOBS, BLOBS, and improperly stored XML</li>
<li>Created Oracle format files and SQL*Loader jobs to load data with high performance</li>
<li>Built dashboards and log pipelines for audit, QA, and client review</li>
</ul>
<hr/>
<h2>Challenges &amp; Solutions</h2>
<table>
<tr>
<th>Issue</th>
<th>Root Cause</th>
<th>Solution</th>
</tr>
<tr>
<td>Inconsistent Date Format</td>
<td>Assembler intercept on DB2 inserted dates in DDMMYY format</td>
<td>Developed custom Oracle function + query layer logic to reformat dates before insertion</td>
</tr>
<tr>
<td>Encoding Corruption</td>
<td>Improper EBCDIC ‚Üí UTF-8 conversion introduced special characters</td>
<td>Moved conversion to mainframe-side using DSN Utility for clean encoding</td>
</tr>
<tr>
<td>Sort Order Mismatch</td>
<td>Mainframe vs. Oracle sort behavior differed without ORDER BY clause</td>
<td>Asked developers to include explicit ORDER BY clauses to mitigate discrepancies</td>
</tr>
<tr>
<td>Junk Characters in LOB/XML</td>
<td>Character data stored in binary/LOB columns on DB2</td>
<td>Implemented cleansing logic using staging server and validation rules before Oracle load</td>
</tr>
</table>
<hr/>
<h2>Technical Deep Dives</h2>

<!-- CHAR Padding Issue -->
<div class="tech-section">
    <h3>üß© CHAR Padding Issue in PreparedStatements</h3>
    <p><strong>Issue:</strong> Java applications using PreparedStatement queries against migrated CHAR columns returned zero records, while direct SQL execution worked correctly.</p>
    
    <div class="details">
        <h4>Detailed Root Cause:</h4>
        <ul>
            <li>DB2 and Oracle handle CHAR(n) comparisons fundamentally differently:
                <ul>
                    <li>DB2 treats trailing spaces as insignificant in comparisons</li>
                    <li>Oracle maintains trailing space significance in PreparedStatements</li>
                    <li>This caused silent failures - no errors, just empty result sets</li>
                </ul>
            </li>
            <li>The issue specifically affected WHERE clauses using parameter binding</li>
            <li>Direct SQL execution worked because explicit spaces were included</li>
        </ul>

        <h4>Example Table Structure:</h4>
        <pre><code class="language-sql">CREATE TABLE TST_PRODUCT (
    ID    NUMBER(3,0),
    NAME  CHAR(20)
);</code></pre>

        <h4>Problem Demonstration:</h4>
        <pre><code class="language-java">// This works - returns records
Statement stmt = conn.createStatement();
ResultSet rs = stmt.executeQuery(
    "SELECT ID, NAME FROM TST_PRODUCT " +
    "WHERE NAME = 'Apple               '"
);

// This fails - returns 0 records
PreparedStatement ps = conn.prepareStatement(
    "SELECT ID, NAME FROM TST_PRODUCT WHERE NAME = ?"
);
ps.setString(1, "Apple");
ResultSet rs = ps.executeQuery();</code></pre>

        <h4>Analysis & Impact:</h4>
        <ul>
            <li>Issue affected all JDBC applications using PreparedStatements</li>
            <li>Risk of data inconsistency in search operations</li>
            <li>Performance impact from missed index usage</li>
            <li>Third-party tools and ORMs also affected</li>
        </ul>

        <h4>Solution Implementation:</h4>
        <pre><code class="language-java">// Short-term fix using RPAD
PreparedStatement ps = conn.prepareStatement(
    "SELECT ID, NAME FROM TST_PRODUCT " +
    "WHERE NAME = RPAD(?, 20)"
);
ps.setString(1, "Apple");</code></pre>

        <h4>Long-term Resolution Steps:</h4>
        <ul>
            <li>Generated inventory of all CHAR columns using metadata queries</li>
            <li>Created analysis script to scan codebase for affected SQL</li>
            <li>Implemented systematic conversion plan:
                <ul>
                    <li>Convert CHAR ‚Üí VARCHAR2 where possible</li>
                    <li>Add explicit RPAD calls where conversion not feasible</li>
                    <li>Update application configurations and ORM settings</li>
                </ul>
            </li>
        </ul>

        <h4>References:</h4>
        <ul>
            <li>Oracle Documentation: CHAR Type Comparison Rules</li>
            <li>Related StackOverflow discussions on CHAR + PreparedStatement issues</li>
            <li>Internal migration guidelines updated with CHAR handling section</li>
        </ul>
    </div>
</div>

<!-- Date Format Issues -->
<div class="tech-section">
    <h3>üßÆ Date Format Standardization</h3>
    <p><strong>Issue:</strong> Multiple date formats causing SQL*Loader failures and data corruption.</p>
    
    <div class="details">
        <h4>Supported Input Formats:</h4>
        <table>
            <tr>
                <th>Format Name</th>
                <th>Abbreviation</th>
                <th>Example</th>
            </tr>
            <tr>
                <td>International Standards (ISO)</td>
                <td>ISO</td>
                <td>2003-12-25</td>
            </tr>
            <tr>
                <td>IBM USA Standard</td>
                <td>USA</td>
                <td>12/25/2003</td>
            </tr>
            <tr>
                <td>IBM European Standard</td>
                <td>EUR</td>
                <td>25.12.2003</td>
            </tr>
            <tr>
                <td>Japanese Industrial Standard</td>
                <td>JIS</td>
                <td>2003-12-25</td>
            </tr>
        </table>

        <h4>Function Implementation:</h4>
        <pre><code class="language-sql">create or replace function clean_date
  (p_date_str in varchar2)
  return date
is
  l_dt_fmt_nt sys.dbms_debug_vc2coll := sys.dbms_debug_vc2coll(
    'DD-MM-YYYY', 'DD.MM.YYYY', 
    'YYYY-MM-DD', 'YYYY.MM.DD'
  );
  return_value date;
begin
  for idx in l_dt_fmt_nt.first()..l_dt_fmt_nt.last() loop
    begin
      return_value := to_date(p_date_str, l_dt_fmt_nt(idx));
      exit;
    exception
      when others then null;
    end;
  end loop;
  return return_value;
end clean_date;</code></pre>

        <h4>Sample Usage:</h4>
        <pre><code class="language-sql">-- All return '25-01-2022'
SELECT clean_date('2022-01-25') FROM dual;
SELECT clean_date('25.01.2022') FROM dual;
SELECT clean_date('01/25/2022') FROM dual;</code></pre>

        <h4>Implementation Notes:</h4>
        <ul>
            <li>Returns NULL if no format matches the input string</li>
            <li>Uses TO_DATE internally for format validation</li>
            <li>Originally named normalize_date, renamed to clean_date for clarity</li>
            <li>Successfully deployed across all environments (Dev, Test, ACC, Prod)</li>
            <li>Performance optimized by exiting on first successful match</li>
        </ul>

        <h4>Integration Points:</h4>
        <ul>
            <li>Used in SQL*Loader control files for data validation</li>
            <li>Integrated with Java applications via JDBC</li>
            <li>Added to data quality dashboards for monitoring</li>
            <li>Included in migration documentation and handover</li>
        </ul>
    </div>
</div>

<!-- IDENTITY Column Issues -->
<div class="tech-section">
    <h3>üîë IDENTITY Column Migration</h3>
    <p><strong>Issue:</strong> Oracle rejected inserts into GENERATED ALWAYS AS IDENTITY columns.</p>
    
    <div class="details">
        <h4>Technical Background:</h4>
        <ul>
            <li>Oracle blocks all inserts into GENERATED ALWAYS columns</li>
            <li>No equivalent to SQL Server's IDENTITY_INSERT</li>
            <li>Need to preserve original key values during migration</li>
        </ul>

        <h4>Solution Workflow:</h4>
        <pre><code class="language-sql">-- Pre-load modification
ALTER TABLE target_table MODIFY id_col 
    GENERATED BY DEFAULT AS IDENTITY;

-- Load data...

-- Post-load restoration
ALTER TABLE target_table MODIFY id_col 
    GENERATED ALWAYS AS IDENTITY;</code></pre>
    </div>
</div>

<!-- SYSOUT Issues -->
<div class="tech-section">
    <h3>üìé SYSOUT ABEND Handling</h3>
    <p><strong>Issue:</strong> Large unload jobs failing with ABEND 722.</p>
    
    <div class="details">
        <h4>Root Cause:</h4>
        <ul>
            <li>SYSOUT exceeded 100,001 lines limit</li>
            <li>Debug logs flooding output stream</li>
            <li>No early warning mechanism</li>
        </ul>

        <h4>JCL Solution:</h4>
        <pre><code class="language-jcl">//UNLOAD   EXEC PGM=DSNUTILB
//SYSOUT   DD SYSOUT=*,OUTLIM=900000
//SYSPRINT DD SYSOUT=*
//SYSERR   DD SYSOUT=*</code></pre>
    </div>
</div>

<!-- Special Characters -->
<div class="tech-section">
    <h3>üåç Character Encoding Resolution</h3>
    <p><strong>Issue:</strong> Special characters corrupted during transfer (|, ¬¢, [).</p>
    
    <div class="details">
        <h4>Character Mapping Analysis:</h4>
        <pre><code>Code Point  CCSID 037   CCSID 500
X'4F'      |           `
X'5A'      !           ]
X'BA'      ¬¨           `</code></pre>

        <h4>Solution Components:</h4>
        <ul>
            <li>Standardized on UTF-8 throughout pipeline</li>
            <li>Added ENCODING UNICODE parameter to DSNUTILB</li>
            <li>Implemented hex-level validation checks</li>
        </ul>
    </div>
</div>

<!-- Control File Versioning -->
<div class="tech-section">
    <h3>üìÑ Control File Version Evolution</h3>
    <p><strong>Issue:</strong> SQL*Loader control files needed progressive enhancement for performance, encoding, and constraint handling.</p>
    
    <div class="details">
        <h4>Version History:</h4>
        <table>
            <tr>
                <th>Version</th>
                <th>Date</th>
                <th>Key Changes</th>
            </tr>
            <tr>
                <td>V1</td>
                <td>06.12.21</td>
                <td>Initial version - base loader control files</td>
            </tr>
            <tr>
                <td>V2</td>
                <td>28.02.22</td>
                <td>
                    <ul>
                        <li>Added DIRECT=TRUE for performance</li>
                        <li>Set CHARACTERSET AL32UTF8</li>
                        <li>Added TRAILING NULLCOLS</li>
                    </ul>
                </td>
            </tr>
            <tr>
                <td>V3</td>
                <td>12.04.22</td>
                <td>
                    <ul>
                        <li>Added LENGTH SEMANTICS CHAR</li>
                        <li>Fixed TIMESTAMP handling in PARTTIME columns</li>
                    </ul>
                </td>
            </tr>
            <tr>
                <td>V4</td>
                <td>24.08.22</td>
                <td>Added REENABLE for auto FK constraint restoration</td>
            </tr>
        </table>

        <h4>Key SQL*Loader Options:</h4>
        <table>
            <tr>
                <th>Option</th>
                <th>Purpose</th>
                <th>Impact</th>
            </tr>
            <tr>
                <td><code>DIRECT=TRUE</code></td>
                <td>Enables direct-path loading</td>
                <td>
                    <ul>
                        <li>Bypasses SQL layer for faster inserts</li>
                        <li>60-80% performance improvement</li>
                        <li>Reduced redo generation</li>
                    </ul>
                </td>
            </tr>
            <tr>
                <td><code>CHARACTERSET AL32UTF8</code></td>
                <td>UTF-8 character encoding</td>
                <td>
                    <ul>
                        <li>Full Unicode support</li>
                        <li>Compatible with mainframe EBCDIC conversion</li>
                        <li>Handles special characters correctly</li>
                    </ul>
                </td>
            </tr>
            <tr>
                <td><code>LENGTH SEMANTICS CHAR</code></td>
                <td>Character-based length calculation</td>
                <td>
                    <ul>
                        <li>Consistent with DB2's length behavior</li>
                        <li>Prevents truncation of multi-byte chars</li>
                        <li>Required for CJK character support</li>
                    </ul>
                </td>
            </tr>
            <tr>
                <td><code>TRUNCATE</code></td>
                <td>Pre-load table cleanup</td>
                <td>
                    <ul>
                        <li>Faster than DELETE</li>
                        <li>Resets storage allocation</li>
                        <li>Maintains table structure</li>
                    </ul>
                </td>
            </tr>
            <tr>
                <td><code>TRAILING NULLCOLS</code></td>
                <td>Flexible column handling</td>
                <td>
                    <ul>
                        <li>Allows partial data loads</li>
                        <li>Compatible with variable-length records</li>
                        <li>Reduces data validation errors</li>
                    </ul>
                </td>
            </tr>
        </table>

        <h4>Sample Control File Implementation:</h4>
        <pre><code class="language-sql">OPTIONS (DIRECT=TRUE, SKIP=0, ERRORS=0)
LOAD DATA
CHARACTERSET AL32UTF8
LENGTH SEMANTICS CHAR
INFILE 'data_file.dat'
BADFILE 'data_file.bad'
DISCARDFILE 'data_file.dsc'
TRUNCATE INTO TABLE target_table
FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '"'
TRAILING NULLCOLS
(
    id        POSITION(1:10),
    name      CHAR(50),
    birthdate DATE "YYYY-MM-DD",
    status    DECODE(:"STATUS",
                    'A', 'ACTIVE',
                    'I', 'INACTIVE',
                    'UNKNOWN')
)</code></pre>

        <h4>Performance Notes:</h4>
        <ul>
            <li>Direct path loading achieved ~70% faster load times</li>
            <li>TRAILING NULLCOLS reduced data validation errors by 45%</li>
            <li>CHARACTER SEMANTICS eliminated multi-byte truncation issues</li>
            <li>TRUNCATE vs DELETE showed 3x faster table preparation</li>
        </ul>

        <h4>Integration Notes:</h4>
        <ul>
            <li>Control files versioned in source control</li>
            <li>Automated testing for each version upgrade</li>
            <li>Version mapping maintained in migration documentation</li>
            <li>Rollback procedures documented for each enhancement</li>
        </ul>
    </div>
</div>

<!-- Mainframe Unload Process -->
<div class="tech-section">
    <h3>üì¶ Mainframe Data Unload Process</h3>
    <p><strong>Issue:</strong> Need to extract large volumes of DB2 z/OS data while maintaining data integrity and handling EBCDIC encoding.</p>
    
    <div class="details">
        <h4>DSNUTILB Configuration:</h4>
        <pre><code class="language-jcl">//UDPRC1  PROC
//* STEP 1: UNLOAD TABLE DCVONOP.TVB0_ELEMENT
//STEP1   EXEC PGM=DSNUTILB,PARM=(DACC,'UNLOAD')
//STEPLIB  DD DSN=SYS1.DSNDPRD.P1.SDSNLOAD,DISP=SHR
//SYSREC   DD DSN=TEST.RVB0JRA.UNLOAD.TVB0.ELEMENT,
//            DISP=(NEW,CATLG,CATLG),
//            UNIT=SYSDA,SPACE=(CYL,(50,500))
//SYSPUNCH DD SYSOUT=*
//SYSPRINT DD SYSOUT=*
//UTPRINT  DD SYSOUT=*
//SYSUDUMP DD SYSOUT=*
//SYSIN    DD *
 UNLOAD DATA
 FROM TABLE DCVONOP.TVB0_ELEMENT
 DELIMITED CHARDEL "" COLDEL ','</code></pre>

        <h4>Key Parameters:</h4>
        <table>
            <tr>
                <th>Parameter</th>
                <th>Value</th>
                <th>Purpose</th>
            </tr>
            <tr>
                <td><code>PGM=DSNUTILB</code></td>
                <td>DSNUTILB</td>
                <td>IBM's high-performance DB2 utility for batch operations</td>
            </tr>
            <tr>
                <td><code>PARM=(DACC,'UNLOAD')</code></td>
                <td>DACC + UNLOAD</td>
                <td>Specifies DB2 subsystem and unload function</td>
            </tr>
            <tr>
                <td><code>SPACE=(CYL,(50,500))</code></td>
                <td>50 primary, 500 secondary</td>
                <td>Dynamic space allocation in cylinders</td>
            </tr>
            <tr>
                <td><code>DELIMITED</code></td>
                <td>COLDEL=',' CHARDEL=""</td>
                <td>CSV format without string delimiters</td>
            </tr>
        </table>

        <h4>Optimization Techniques:</h4>
        <ul>
            <li>Used parallel unload jobs for tables > 1GB</li>
            <li>Implemented REORG before unload for fragmented tables</li>
            <li>Monitored DB2 catalog statistics for space estimation</li>
            <li>Added integrity checks via COUNT(*) validation</li>
        </ul>

        <h4>Performance Stats:</h4>
        <ul>
            <li>Average unload speed: ~500MB/minute</li>
            <li>Parallel jobs reduced total time by 65%</li>
            <li>Space estimation accuracy: 95%</li>
            <li>Zero data integrity issues reported</li>
        </ul>

        <h4>Error Handling:</h4>
        <pre><code class="language-jcl">//SYSUDUMP DD SYSOUT=*   /* For abend diagnostics */
//UTPRINT  DD SYSOUT=*   /* For utility messages */
//SYSPRINT DD SYSOUT=*   /* For detailed progress */</code></pre>

        <h4>Integration Points:</h4>
        <ul>
            <li>JCL procedures stored in version control</li>
            <li>Automated job submission via Control-M</li>
            <li>Output validated by downstream Oracle processes</li>
            <li>Logging integrated with central monitoring</li>
        </ul>
    </div>
</div>

<hr/>
<h2>Validation Strategy</h2>
<p>
      I implemented a rigorous 3-step validation framework to ensure accurate and auditable data migration:
    </p>
<ol>
<li><strong>Row Count Matching</strong>: between source unload, transferred file, and Oracle load.</li>
<li><strong>SHA-256 Fingerprint Validation</strong>: for each dataset at unload, transfer, and post-load stages.</li>
<li><strong>Conditional Deep Comparison</strong>: only run if hash mismatch occurred, pinpointed row/column differences.</li>
</ol>
<p>
      This strategy helped reduce unnecessary full-file comparisons while maintaining high confidence 
      and compliance with audit policies.
    </p>
<img alt="IMG EB16CD62-506C-4F29-8B4A-AAA5058DEC75 cleaned" src="IMG_EB16CD62-506C-4F29-8B4A-AAA5058DEC75_cleaned.jpg"/><img alt="IMG 21EB7982-B371-47C6-841B-4C4CADD4ABCE cleaned" src="IMG_21EB7982-B371-47C6-841B-4C4CADD4ABCE_cleaned.jpg"/><img alt="IMG DCCF0B5A-E185-4300-9103-ACAE7E59CCB2 cleaned" src="IMG_DCCF0B5A-E185-4300-9103-ACAE7E59CCB2_cleaned.jpg"/><img alt="IMG FEA7506B-1058-48E9-BE9A-4D4934E91EED cleaned" src="IMG_FEA7506B-1058-48E9-BE9A-4D4934E91EED_cleaned.jpg"/><img alt="IMG 06F66020-FCD6-4355-984E-2617D2789B57 cleaned" src="IMG_06F66020-FCD6-4355-984E-2617D2789B57_cleaned.jpg"/><img alt="IMG E5002EA7-1A3E-47E1-A1FB-BAE1631BC233 cleaned" src="IMG_E5002EA7-1A3E-47E1-A1FB-BAE1631BC233_cleaned.jpg"/><img alt="IMG F2A33DF7-EE4F-4247-9FE7-0615E6DA21E5 cleaned" src="IMG_F2A33DF7-EE4F-4247-9FE7-0615E6DA21E5_cleaned.jpg"/><img alt="IMG 583AEBF5-46E6-46BB-950E-4A35F4553AAC cleaned" src="IMG_583AEBF5-46E6-46BB-950E-4A35F4553AAC_cleaned.jpg"/><hr/>
<h2>Results</h2>
<ul>
<li>Successful migration of 2500+ GB of production data</li>
<li>Three major go-lives completed across LPARs without incident</li>
<li>Audit-approved logs and artifacts produced for every data movement</li>
<li>Reusable tooling created for use in future mainframe exit programs</li>
<li>Database performance tuning and index optimization post-migration</li>
</ul>
<hr/>
<h2>Lessons Learned</h2>
<p>
      Legacy modernization is not just about converting data formats‚Äîit‚Äôs about understanding how old systems worked 
      in production, and creating solutions that preserve business logic and performance behavior. Most importantly, 
      migration must be auditable and repeatable.
      Also it is not just about COBOL to JAVA conversion, or DB2 to Oracle migration, these are the tip of an iceberg. 
      The real challenge lies in the details: data formats, encoding issues, sort orders, and special data types like LOBs and XML.
      Also in the related integration points like assembers, custom implementaiton, batch jobs, schedulers , End user Developed Applications, etc.
    </p>
<p>
      This project taught me how to think beyond code‚Äîto architect systems, lead teams, and deliver on timelines 
      where both performance and integrity are non-negotiable.
    </p>
<br/><br/>
<a href="../../index.html">‚Üê Back to Home</a>
</div>
</body>
</html>

